{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\"\n",
    "#module_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\n",
    "\n",
    "bert_layer = hub.KerasLayer(module_url, trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'BERT_Easy_Implementation' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/tkeldenich/BERT_Easy_Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Now, I won't deny that when I purchased this o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The saddest thing about this \"tribute\" is that...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Last night I decided to watch the prequel or s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  Now, I won't deny that when I purchased this o...      0\n",
       "1  The saddest thing about this \"tribute\" is that...      0\n",
       "2  Last night I decided to watch the prequel or s...      0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv(\"./BERT_Easy_Implementation/data/train.csv\")\n",
    "test = pd.read_csv(\"./BERT_Easy_Implementation/data/test.csv\")\n",
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def bert_encode(texts, tokenizer, max_len=512):\n",
    "    all_tokens = []\n",
    "    all_masks = []\n",
    "    all_segments = []\n",
    "\n",
    "    for text in texts:\n",
    "        text = tokenizer.tokenize(text)\n",
    "\n",
    "        text = text[:max_len-2]\n",
    "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
    "        pad_len = max_len - len(input_sequence)\n",
    "\n",
    "        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
    "        tokens += [0] * pad_len\n",
    "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "        segment_ids = [0] * max_len\n",
    "\n",
    "        all_tokens.append(tokens)\n",
    "        all_masks.append(pad_masks)\n",
    "        all_segments.append(segment_ids)\n",
    "\n",
    "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\arthu\\OneDrive\\CentraleSupelec\\PoleIA\\bert.ipynb Cell 10'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/arthu/OneDrive/CentraleSupelec/PoleIA/bert.ipynb#ch0000019?line=0'>1</a>\u001b[0m train_input \u001b[39m=\u001b[39m bert_encode(train\u001b[39m.\u001b[39;49mtext\u001b[39m.\u001b[39;49mvalues, tokenizer, max_len\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/arthu/OneDrive/CentraleSupelec/PoleIA/bert.ipynb#ch0000019?line=1'>2</a>\u001b[0m test_input \u001b[39m=\u001b[39m bert_encode(test\u001b[39m.\u001b[39mtext\u001b[39m.\u001b[39mvalues, tokenizer, max_len\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/arthu/OneDrive/CentraleSupelec/PoleIA/bert.ipynb#ch0000019?line=2'>3</a>\u001b[0m train_labels \u001b[39m=\u001b[39m train\u001b[39m.\u001b[39mlabel\u001b[39m.\u001b[39mvalues\n",
      "\u001b[1;32mc:\\Users\\arthu\\OneDrive\\CentraleSupelec\\PoleIA\\bert.ipynb Cell 8'\u001b[0m in \u001b[0;36mbert_encode\u001b[1;34m(texts, tokenizer, max_len)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/arthu/OneDrive/CentraleSupelec/PoleIA/bert.ipynb#ch0000016?line=9'>10</a>\u001b[0m all_segments \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/arthu/OneDrive/CentraleSupelec/PoleIA/bert.ipynb#ch0000016?line=11'>12</a>\u001b[0m \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m texts:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/arthu/OneDrive/CentraleSupelec/PoleIA/bert.ipynb#ch0000016?line=12'>13</a>\u001b[0m     text \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39;49mtokenize(text)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/arthu/OneDrive/CentraleSupelec/PoleIA/bert.ipynb#ch0000016?line=14'>15</a>\u001b[0m     text \u001b[39m=\u001b[39m text[:max_len\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/arthu/OneDrive/CentraleSupelec/PoleIA/bert.ipynb#ch0000016?line=15'>16</a>\u001b[0m     input_sequence \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39m[CLS]\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m text \u001b[39m+\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39m[SEP]\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\tokenization_utils.py:546\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.tokenize\u001b[1;34m(self, text, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/arthu/AppData/Local/Programs/Python/Python38/lib/site-packages/transformers/tokenization_utils.py?line=543'>544</a>\u001b[0m         tokenized_text\u001b[39m.\u001b[39mappend(token)\n\u001b[0;32m    <a href='file:///c%3A/Users/arthu/AppData/Local/Programs/Python/Python38/lib/site-packages/transformers/tokenization_utils.py?line=544'>545</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/arthu/AppData/Local/Programs/Python/Python38/lib/site-packages/transformers/tokenization_utils.py?line=545'>546</a>\u001b[0m         tokenized_text\u001b[39m.\u001b[39mextend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tokenize(token))\n\u001b[0;32m    <a href='file:///c%3A/Users/arthu/AppData/Local/Programs/Python/Python38/lib/site-packages/transformers/tokenization_utils.py?line=546'>547</a>\u001b[0m \u001b[39m# [\"This\", \" is\", \" something\", \"<special_token_1>\", \"else\"]\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/arthu/AppData/Local/Programs/Python/Python38/lib/site-packages/transformers/tokenization_utils.py?line=547'>548</a>\u001b[0m \u001b[39mreturn\u001b[39;00m tokenized_text\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\models\\bert\\tokenization_bert.py:224\u001b[0m, in \u001b[0;36mBertTokenizer._tokenize\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/arthu/AppData/Local/Programs/Python/Python38/lib/site-packages/transformers/models/bert/tokenization_bert.py?line=221'>222</a>\u001b[0m split_tokens \u001b[39m=\u001b[39m []\n\u001b[0;32m    <a href='file:///c%3A/Users/arthu/AppData/Local/Programs/Python/Python38/lib/site-packages/transformers/models/bert/tokenization_bert.py?line=222'>223</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdo_basic_tokenize:\n\u001b[1;32m--> <a href='file:///c%3A/Users/arthu/AppData/Local/Programs/Python/Python38/lib/site-packages/transformers/models/bert/tokenization_bert.py?line=223'>224</a>\u001b[0m     \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbasic_tokenizer\u001b[39m.\u001b[39;49mtokenize(text, never_split\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mall_special_tokens):\n\u001b[0;32m    <a href='file:///c%3A/Users/arthu/AppData/Local/Programs/Python/Python38/lib/site-packages/transformers/models/bert/tokenization_bert.py?line=224'>225</a>\u001b[0m \n\u001b[0;32m    <a href='file:///c%3A/Users/arthu/AppData/Local/Programs/Python/Python38/lib/site-packages/transformers/models/bert/tokenization_bert.py?line=225'>226</a>\u001b[0m         \u001b[39m# If the token is part of the never_split set\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/arthu/AppData/Local/Programs/Python/Python38/lib/site-packages/transformers/models/bert/tokenization_bert.py?line=226'>227</a>\u001b[0m         \u001b[39mif\u001b[39;00m token \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbasic_tokenizer\u001b[39m.\u001b[39mnever_split:\n\u001b[0;32m    <a href='file:///c%3A/Users/arthu/AppData/Local/Programs/Python/Python38/lib/site-packages/transformers/models/bert/tokenization_bert.py?line=227'>228</a>\u001b[0m             split_tokens\u001b[39m.\u001b[39mappend(token)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\models\\bert\\tokenization_bert.py:411\u001b[0m, in \u001b[0;36mBasicTokenizer.tokenize\u001b[1;34m(self, text, never_split)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/arthu/AppData/Local/Programs/Python/Python38/lib/site-packages/transformers/models/bert/tokenization_bert.py?line=408'>409</a>\u001b[0m         \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrip_accents:\n\u001b[0;32m    <a href='file:///c%3A/Users/arthu/AppData/Local/Programs/Python/Python38/lib/site-packages/transformers/models/bert/tokenization_bert.py?line=409'>410</a>\u001b[0m             token \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_strip_accents(token)\n\u001b[1;32m--> <a href='file:///c%3A/Users/arthu/AppData/Local/Programs/Python/Python38/lib/site-packages/transformers/models/bert/tokenization_bert.py?line=410'>411</a>\u001b[0m     split_tokens\u001b[39m.\u001b[39mextend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_split_on_punc(token, never_split))\n\u001b[0;32m    <a href='file:///c%3A/Users/arthu/AppData/Local/Programs/Python/Python38/lib/site-packages/transformers/models/bert/tokenization_bert.py?line=412'>413</a>\u001b[0m output_tokens \u001b[39m=\u001b[39m whitespace_tokenize(\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(split_tokens))\n\u001b[0;32m    <a href='file:///c%3A/Users/arthu/AppData/Local/Programs/Python/Python38/lib/site-packages/transformers/models/bert/tokenization_bert.py?line=413'>414</a>\u001b[0m \u001b[39mreturn\u001b[39;00m output_tokens\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\models\\bert\\tokenization_bert.py:437\u001b[0m, in \u001b[0;36mBasicTokenizer._run_split_on_punc\u001b[1;34m(self, text, never_split)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/arthu/AppData/Local/Programs/Python/Python38/lib/site-packages/transformers/models/bert/tokenization_bert.py?line=434'>435</a>\u001b[0m \u001b[39mwhile\u001b[39;00m i \u001b[39m<\u001b[39m \u001b[39mlen\u001b[39m(chars):\n\u001b[0;32m    <a href='file:///c%3A/Users/arthu/AppData/Local/Programs/Python/Python38/lib/site-packages/transformers/models/bert/tokenization_bert.py?line=435'>436</a>\u001b[0m     char \u001b[39m=\u001b[39m chars[i]\n\u001b[1;32m--> <a href='file:///c%3A/Users/arthu/AppData/Local/Programs/Python/Python38/lib/site-packages/transformers/models/bert/tokenization_bert.py?line=436'>437</a>\u001b[0m     \u001b[39mif\u001b[39;00m _is_punctuation(char):\n\u001b[0;32m    <a href='file:///c%3A/Users/arthu/AppData/Local/Programs/Python/Python38/lib/site-packages/transformers/models/bert/tokenization_bert.py?line=437'>438</a>\u001b[0m         output\u001b[39m.\u001b[39mappend([char])\n\u001b[0;32m    <a href='file:///c%3A/Users/arthu/AppData/Local/Programs/Python/Python38/lib/site-packages/transformers/models/bert/tokenization_bert.py?line=438'>439</a>\u001b[0m         start_new_word \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_input = bert_encode(train.text.values, tokenizer, max_len=100)\n",
    "test_input = bert_encode(test.text.values, tokenizer, max_len=100)\n",
    "train_labels = train.label.values\n",
    "test_labels = test.label.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "def build_model(bert_layer, max_len=512):\n",
    "    input_word_ids = Input(\n",
    "        shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
    "    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
    "\n",
    "    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "    clf_output = sequence_output[:, 0, :]\n",
    "    out = Dense(1, activation='sigmoid')(clf_output)\n",
    "\n",
    "    model = Model(inputs=[input_word_ids, input_mask,\n",
    "                  segment_ids], outputs=out)\n",
    "    model.compile(Adam(lr=2e-6), loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = build_model(bert_layer, max_len=100)\n",
    "\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "train_history = model.fit(\n",
    "    train_input, train_labels,\n",
    "    validation_split=0.2,\n",
    "    epochs=5,\n",
    "    batch_size=32\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f499c09746ed89bb16a1ef7cbb581cc63f8572953d5a366b82ca25faa5a00be4"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
